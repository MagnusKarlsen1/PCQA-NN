{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit, vmap\n",
    "import torch\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.spatial import KDTree\n",
    "import os\n",
    "import random\n",
    "import numpy.linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def preprocess(points):\n",
    "    mean_p = points.mean(axis=0)\n",
    "    min_p, max_p = jnp.min(points, axis=0), jnp.max(points, axis=0)\n",
    "    bbdiag = jnp.linalg.norm(max_p - min_p, ord=2) # Bounding box diagonal L2 norm (Euclidean distance)\n",
    "    return (points - mean_p) / (0.5 * bbdiag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_points(patch_points):\n",
    "    '''\n",
    "    Args:\n",
    "        patch_points: xyz points\n",
    "\n",
    "    Returns:\n",
    "        patch_points: xyz points after aligning using pca\n",
    "    '''\n",
    "    # compute pca of points in the patch:\n",
    "    # center the patch around the mean:\n",
    "    pts_mean = patch_points.mean(0)\n",
    "    patch_points = patch_points - pts_mean\n",
    "    trans, _, _ = torch.svd(torch.t(patch_points))\n",
    "    patch_points = torch.mm(patch_points, trans)\n",
    "    cp_new = -pts_mean  # since the patch was originally centered, the original cp was at (0,0,0)\n",
    "    cp_new = torch.matmul(cp_new, trans)\n",
    "    # re-center on original center point\n",
    "    patch_points = patch_points - cp_new\n",
    "    return patch_points, trans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_neighborhood_to_txt(patch_points, filename=\"neighborhood.txt\"):\n",
    "    np.savetxt(filename, patch_points, fmt=\"%.6f\", delimiter=\" \")\n",
    "    print(f\"Saved neighborhood to {filename}\")\n",
    "    \n",
    "    \n",
    "## Modified leihui code to save the files \n",
    "    \n",
    "def processPartL(kdtree, index, points, searchK):\n",
    "    # print (f'points[index, :]:{points[index, :]}')\n",
    "    point_distances, patch_point_inds = kdtree.query(points[index, :], k=searchK)\n",
    "    rad = max(point_distances)\n",
    "    patch_points = torch.from_numpy(points[patch_point_inds, :])\n",
    "    \n",
    "    # center the points around the query point and scale patch to unit sphere\n",
    "    patch_points = patch_points - torch.from_numpy(points[index, :])\n",
    "    # patch_points = patch_points / rad\n",
    "    \n",
    "    patch_points, trans = pca_points(patch_points)\n",
    "    return patch_points, patch_point_inds, trans, rad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PC-Diff imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdiff import knn_graph, estimate_basis, build_grad_div\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_gradients(pointcloud, k_neighbors):\n",
    "    #TODO: Find out if this is only grad_x or just the gradient found here, maybe need more info for complete gradient\n",
    "    edge_index = knn_graph(pointcloud, k_neighbors)\n",
    "    normal, x_basis, y_basis = estimate_basis(pointcloud, edge_index)\n",
    "    gradients = build_grad_ours(pointcloud, normal, x_basis, y_basis, edge_index)\n",
    "    \n",
    "    return gradients \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdiff import coords_projected, gaussian_weights, weighted_least_squares\n",
    "\n",
    "def build_grad_ours(pos, normal, x_basis, y_basis, edge_index, kernel_width=1, regularizer=1e-8, shape_regularizer=None):\n",
    "    row, col = edge_index\n",
    "    k = (row == 0).sum()\n",
    "\n",
    "    coords = coords_projected(pos, normal, x_basis, y_basis, edge_index, k)\n",
    "\n",
    "    dist = LA.norm(pos[col] - pos[row], axis=1)\n",
    "    weights = gaussian_weights(dist, k, kernel_width)\n",
    "\n",
    "    if shape_regularizer is None:\n",
    "        wls = weighted_least_squares(coords, weights, k, regularizer)\n",
    "    else:\n",
    "        wls, wls_shape = weighted_least_squares(coords, weights, k, regularizer, shape_regularizer)\n",
    "\n",
    "    gradients = wls[::20,1:3]    \n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating function to generate datasets from pointclouds\n",
    "* Pointclouds in the dataset folder are all used, divided to use equal amount of neighborhoods from each cloud\n",
    "* Can be used to save multiple txt files containing a point cloud neighborhood each\n",
    "* Can be used to create one long txt file where the neighborhoods are appended to the file\n",
    "* The data will have the structure (x, y, z, gradient_x, gradient_y, radius, num points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(num_training_sets, num_neighbors, grad_neighbors, method = \"PCA\", datasets = \"./Data/Full_point_clouds\", save_path = \"./Data/Training_data\", save_to_file = False,\n",
    "                         single_file_name=\"CombinedDataset.txt\", save_mode=\"single\"):\n",
    "    \n",
    "    # check if the save folder is available or create it if not\n",
    "    if save_to_file and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    all_files = [os.path.join(datasets, f) for f in os.listdir(datasets) if f.endswith('.txt') or f.endswith('.xyz')]\n",
    "    total_sets_created = 0 # Counter to see how many training points have been created \n",
    "    \n",
    "    sets_per_file = int(num_training_sets/len(all_files))\n",
    "    \n",
    "    for file_path in all_files:\n",
    "        if total_sets_created >= num_training_sets: \n",
    "            break # Break if the number of training data created is reached\n",
    "    \n",
    "    if save_mode == \"single\":\n",
    "        single_file_path = os.path.join(save_path, single_file_name)\n",
    "        if os.path.exists(single_file_path):\n",
    "            os.remove(single_file_path)  # Clear the file if it exists\n",
    "    \n",
    "        \n",
    "        \n",
    "    for file_path in all_files:\n",
    "        rawpoints = np.loadtxt(file_path, usecols=(0, 1, 2))  # Load only the first three columns (x, y, z)\n",
    "        print(f\"Loaded {rawpoints.shape[0]} points from {file_path}\")\n",
    "\n",
    "        points = preprocess(rawpoints)\n",
    "        points_np = np.array(points.block_until_ready())\n",
    "        kdtree = KDTree(points_np)\n",
    "\n",
    "        gradients = calculate_gradients(points_np, grad_neighbors)\n",
    "        \n",
    "        selected_indices = random.sample(range(len(points)), sets_per_file)\n",
    "        \n",
    "        for i in selected_indices:\n",
    "            if method == \"PCA\":\n",
    "                neighborhood, indices, _, radius = processPartL(kdtree, i, points_np, num_neighbors)\n",
    "                \n",
    "                neighborhood_gradients = gradients[indices]\n",
    "                                \n",
    "                distance_from_origin = np.array(L2_norm(neighborhood).block_until_ready())\n",
    "            \n",
    "                grad_diff = np.array(L2_norm(neighborhood_gradients).block_until_ready())\n",
    "                \n",
    "                radius_list = [radius]*num_neighbors\n",
    "                radius_list = np.full((num_neighbors, 1), radius)  # Ensure it is a column vector\n",
    "\n",
    "                neighborhood_with_gradients = np.hstack([neighborhood.numpy(), neighborhood_gradients])\n",
    "                \n",
    "                neighborhood_with_gradients = np.hstack([neighborhood_with_gradients, radius_list])\n",
    "                \n",
    "        \n",
    "                if save_to_file:\n",
    "                    if save_mode == \"multiple\":\n",
    "                        # Save each neighborhood in a separate file\n",
    "                        filename = os.path.join(save_path, f\"neighborhood_{total_sets_created}.txt\")\n",
    "                        save_neighborhood_to_txt(neighborhood_with_gradients, filename)\n",
    "                    elif save_mode == \"single\":\n",
    "                        # Append the neighborhood to a single file\n",
    "                        with open(single_file_path, \"a\") as f:\n",
    "                            np.savetxt(f, neighborhood_with_gradients, fmt=\"%.6f\", delimiter=\" \")\n",
    "\n",
    "                total_sets_created += 1\n",
    "\n",
    "                if total_sets_created >= num_training_sets:\n",
    "                    break  # Exits the inner loop\n",
    "\n",
    "    return neighborhood  # Align this correctly    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L2_norm(nbh, origin_index=0):\n",
    "    # Convert PyTorch tensor to JAX array if needed\n",
    "    if isinstance(nbh, torch.Tensor):\n",
    "        nbh = jax.device_put(nbh.detach().cpu().numpy())  # Convert to JAX array\n",
    "\n",
    "    nbh_jax = jnp.array(nbh)  # Ensure JAX array\n",
    "    origin = nbh_jax[origin_index]  # Get the origin point\n",
    "    dist = jnp.linalg.norm(nbh_jax - origin, axis=1)  # Compute L2 Norm\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points = create_training_data(200, 20, 20, save_to_file=True, save_mode=\"single\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation from previous but creating features for neighborhoods as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def neighborhood_feature_generation(num_runs=None, mixer=False, outputfile=\"Combined_neighborhood_data.txt\", save_path=\"./Data/Training_data\"):\n",
    "    \"\"\"\n",
    "    Generates mixed-sized neighborhoods using create_training_data() and extracts their statistical features.\n",
    "\n",
    "    Parameters:\n",
    "        num_runs (int): Number of times to run create_training_data() with different neighborhood sizes.\n",
    "        mixer (bool): If True, generate mixed neighborhood sizes.\n",
    "        outputfile (str): Name of the output feature file.\n",
    "        save_path (str): Directory where datasets are stored.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    output_path = os.path.join(save_path, outputfile)\n",
    "\n",
    "    if mixer:\n",
    "        print(\"Generating mixed neighborhood sizes...\")\n",
    "\n",
    "        # Clear the output file at the start to avoid appending to old data\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "\n",
    "        neighborhood_sizes = []  # Store number of neighbors for each run\n",
    "\n",
    "        for _ in range(num_runs):\n",
    "            num_neighbors = np.random.randint(5, 35)  # Random neighborhood size per run\n",
    "            neighborhood_sizes.append(num_neighbors)\n",
    "\n",
    "            # Create new training data with 50 samples per iteration\n",
    "            create_training_data(50, num_neighbors, 20, save_to_file=True, save_mode=\"single\")\n",
    "\n",
    "        # Load the full combined dataset (which should have 50 * num_runs neighborhoods)\n",
    "        combined_data_path = os.path.join(save_path, \"CombinedDataset.txt\")\n",
    "        if not os.path.exists(combined_data_path):\n",
    "            print(\"No combined dataset found. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            data = np.loadtxt(combined_data_path)\n",
    "            if data.size == 0:\n",
    "                print(\"Generated dataset is empty. Exiting.\")\n",
    "                return\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return\n",
    "\n",
    "    else: \n",
    "        # If mixer is False, process an existing dataset\n",
    "        data_path = os.path.join(save_path, \"CombinedDataset.txt\")\n",
    "        if not os.path.exists(data_path):\n",
    "            print(f\"File {data_path} not found. Exiting.\")\n",
    "            return\n",
    "\n",
    "        data = np.loadtxt(data_path)\n",
    "        neighborhood_sizes = [20] * (len(data) // 50)  # Assume 20 neighbors if not using mixer\n",
    "\n",
    "    # Identify separator rows (where the first three columns are all 0)\n",
    "    separators = np.where((data[:, 0] == 0) & (data[:, 1] == 0) & (data[:, 2] == 0))[0]\n",
    "\n",
    "    # Store each neighborhood separately\n",
    "    neighborhoods = []\n",
    "    start_idx = 0\n",
    "\n",
    "    for sep in separators:\n",
    "        if start_idx != sep:\n",
    "            neighborhoods.append(data[start_idx:sep])  \n",
    "        start_idx = sep + 1  \n",
    "\n",
    "    if start_idx < len(data):\n",
    "        neighborhoods.append(data[start_idx:])\n",
    "    \n",
    "    with open(output_path, \"w\") as f:\n",
    "        for i, n in enumerate(neighborhoods):\n",
    "            if len(n) == 0:\n",
    "                continue  \n",
    "\n",
    "            mean_col4 = np.mean(n[:, 3])\n",
    "            var_col4 = np.var(n[:, 3])\n",
    "\n",
    "            mean_col5 = np.mean(n[:, 4])\n",
    "            var_col5 = np.var(n[:, 4])\n",
    "\n",
    "            radius = n[0, -1]  \n",
    "\n",
    "\n",
    "            f.write(f\"{mean_col4:.6f} {var_col4:.6f} {mean_col5:.6f} {var_col5:.6f} {radius:.6f} {num_neighbors}\\n\")\n",
    "        \n",
    "    print(f\"Processed {len(neighborhoods)} neighborhoods. Output saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neighborhood_feature_generation(10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def data_to_neighborhood(num_neighbors=10, save_path=\"./Data/Training_data\"):\n",
    "    neighborhood_training_data = []  # Use a list to store rows first\n",
    "\n",
    "    for i in range(num_neighbors):\n",
    "        create_training_data(50, num_neighbors, 20, save_to_file=True, save_mode=\"single\")\n",
    "        data_path = os.path.join(save_path, \"CombinedDataset.txt\")\n",
    "        if not os.path.exists(data_path):\n",
    "            print(f\"File {data_path} not found. Exiting.\")\n",
    "            return\n",
    "\n",
    "        data = np.loadtxt(data_path)\n",
    "        neighborhood_sizes = [20] * (len(data) // 50)  # Assume 20 neighbors if not using mixer\n",
    "\n",
    "        # Identify separator rows (where the first three columns are all 0)\n",
    "        separators = np.where((data[:, 0] == 0) & (data[:, 1] == 0) & (data[:, 2] == 0))[0]\n",
    "        \n",
    "        # Store each neighborhood separately\n",
    "        neighborhoods = []\n",
    "        start_idx = 0\n",
    "\n",
    "        for sep in separators:\n",
    "            if start_idx != sep:\n",
    "                neighborhoods.append(data[start_idx:sep])  \n",
    "            start_idx = sep + 1  \n",
    "\n",
    "        if start_idx < len(data):\n",
    "            neighborhoods.append(data[start_idx:])\n",
    "\n",
    "        for n in neighborhoods:\n",
    "            if len(n) == 0:\n",
    "                continue  \n",
    "\n",
    "            mean_col4 = np.mean(n[:, 3])\n",
    "            var_col4 = np.var(n[:, 3])\n",
    "\n",
    "            mean_col5 = np.mean(n[:, 4])\n",
    "            var_col5 = np.var(n[:, 4])\n",
    "\n",
    "            radius = n[0, -1]  \n",
    "\n",
    "            # Append to list\n",
    "            neighborhood_training_data.append([mean_col4, var_col4, mean_col5, var_col5, radius])\n",
    "\n",
    "    # Convert to NumPy array at the end\n",
    "    neighborhood_training_data = np.array(neighborhood_training_data)\n",
    "    \n",
    "    return neighborhood_training_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New create training data on better data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../pcdiff/grad_curvature.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data_better(num_training_sets, num_neighbors, comparison_size=6, datasets = \"./Data/Full_point_clouds\", save_path = \"./Data/Training_data\", save_to_file = True,\n",
    "                                 single_file_name=\"THEdataset.txt\", save_mode=\"single\"):\n",
    "    \n",
    "    # check if the save folder is available or create it if not \n",
    "    if save_to_file and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    all_files = [os.path.join(datasets, f) for f in os.listdir(datasets) if f.endswith('.txt') or f.endswith('.xyz')]\n",
    "    total_sets_created = 0 # Counter to see how many training points have been created \n",
    "    \n",
    "    sets_per_file = int(num_training_sets/len(all_files))\n",
    "    \n",
    "    for file_path in all_files:\n",
    "        if total_sets_created >= num_training_sets: \n",
    "            break # Break if the number of training data created is reached\n",
    "    \n",
    "    if save_mode == \"single\":\n",
    "        single_file_path = os.path.join(save_path, single_file_name)\n",
    "        if os.path.exists(single_file_path):\n",
    "            os.remove(single_file_path)  # Clear the file if it exists\n",
    "    \n",
    "        \n",
    "        \n",
    "    for file_path in all_files:\n",
    "        points = np.loadtxt(file_path, usecols=(0, 1, 2))  # Load only the first three columns (x, y, z)\n",
    "        print(f\"Loaded {points.shape[0]} points from {file_path}\")\n",
    "\n",
    "        pos_dist, grad_dist, curv = get_nn_data(points, num_neighbors, comparison_size)\n",
    "        \n",
    "        neighborhood_to_save = np.column_stack((pos_dist, grad_dist, curv))\n",
    "        # Define headers\n",
    "        headers = \"Distance1 Distance2 Distance3 Distance4 Distance5 Distance6 Grad1 Grad2 Grad3 Grad4 Grad5 Grad6 Curvature\"\n",
    "\n",
    "\n",
    "        if save_to_file:\n",
    "            write_mode = \"w\" if total_sets_created == 0 else \"a\"  # \"w\" writes header first, \"a\" appends\n",
    "\n",
    "            # Save data with headers only at the beginning\n",
    "            with open(single_file_path, write_mode) as f:\n",
    "                np.savetxt(f, neighborhood_to_save, fmt=\"%.6f\", delimiter=\" \", header=headers if total_sets_created == 0 else \"\", comments='')\n",
    "\n",
    "        total_sets_created += 1\n",
    "\n",
    "        if total_sets_created >= num_training_sets:\n",
    "            break  # Exits the inner loop\n",
    "\n",
    "    return neighborhood_to_save  # Align this correctly    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 35947 points from ./Data/Full_point_clouds\\bunny_order3_normal_beta.txt\n",
      "(35947, 6)\n",
      "(35947, 6)\n",
      "(35947,)\n",
      "Loaded 120982 points from ./Data/Full_point_clouds\\cube-isometric.xyz\n",
      "(120982, 6)\n",
      "(120982, 6)\n",
      "(120982,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   0.07042217,   0.07042254, ...,   0.45012096,\n",
       "          0.68027246, -11.26065299],\n",
       "       [  0.        ,   0.07042254,   0.07042254, ...,   0.04994819,\n",
       "          0.55386084,  -6.69441754],\n",
       "       [  0.        ,   0.0704225 ,   0.07042256, ...,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       ...,\n",
       "       [  0.        ,   0.07042217,   0.07042254, ...,   0.88795829,\n",
       "          0.54334801,   5.24184792],\n",
       "       [  0.        ,   0.06208038,   0.0708435 , ...,   0.49645737,\n",
       "          0.40042239, -10.13771037],\n",
       "       [  0.        ,   0.07042217,   0.07042254, ...,   0.88795829,\n",
       "          0.54334801, -26.80719155]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_training_data_better(2, 20, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def data_to_neighborhood_better(num_neighbors=10, num_sets=10, save_path=\"./Data/Training_data\"):\n",
    "    neighborhood_training_data = []  # Use a list to store rows first\n",
    "\n",
    "    for i in range(num_sets):\n",
    "        create_training_data(50, num_neighbors, 20, save_to_file=True, save_mode=\"single\")\n",
    "        data_path = os.path.join(save_path, \"THEdataset.txt\")\n",
    "        if not os.path.exists(data_path):\n",
    "            print(f\"File {data_path} not found. Exiting.\")\n",
    "            return\n",
    "\n",
    "        data = np.loadtxt(data_path)\n",
    "        # neighborhood_sizes = [20] * (len(data) // 50)  # Assume 20 neighbors if not using mixer\n",
    "\n",
    "        # Identify separator rows (where the first three columns are all 0)\n",
    "        separators = np.where((data[:, 0] == 0) & (data[:, 1] == 0) & (data[:, 2] == 0))[0]\n",
    "        \n",
    "        # Store each neighborhood separately\n",
    "        neighborhoods = []\n",
    "        start_idx = 0\n",
    "\n",
    "        for sep in separators:\n",
    "            if start_idx != sep:\n",
    "                neighborhoods.append(data[start_idx:sep])  \n",
    "            start_idx = sep + 1  \n",
    "\n",
    "        if start_idx < len(data):\n",
    "            neighborhoods.append(data[start_idx:])\n",
    "\n",
    "        for n in neighborhoods:\n",
    "            if len(n) == 0:\n",
    "                continue  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # mean_col4 = np.mean(n[:, 3])\n",
    "            # var_col4 = np.var(n[:, 3])\n",
    "\n",
    "            # mean_col5 = np.mean(n[:, 4])\n",
    "            # var_col5 = np.var(n[:, 4])\n",
    "\n",
    "            # radius = n[0, -1]  \n",
    "\n",
    "            # Append to list\n",
    "            neighborhood_training_data.append([mean_col4, var_col4, mean_col5, var_col5, radius])\n",
    "    \n",
    "    # Convert to NumPy array at the end\n",
    "    neighborhood_training_data = np.array(neighborhood_training_data)\n",
    "    save_neighborhood_to_txt(neighborhood_training_data, filename=\"DEN_HER.txt\")\n",
    "        \n",
    "    return neighborhood_training_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
