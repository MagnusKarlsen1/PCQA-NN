{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit, vmap\n",
    "import torch\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.spatial import KDTree\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def preprocess(points):\n",
    "    mean_p = points.mean(axis=0)\n",
    "    min_p, max_p = jnp.min(points, axis=0), jnp.max(points, axis=0)\n",
    "    bbdiag = jnp.linalg.norm(max_p - min_p, ord=2) # Bounding box diagonal L2 norm (Euclidean distance)\n",
    "    return (points - mean_p) / (0.5 * bbdiag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_points(patch_points):\n",
    "    '''\n",
    "    Args:\n",
    "        patch_points: xyz points\n",
    "\n",
    "    Returns:\n",
    "        patch_points: xyz points after aligning using pca\n",
    "    '''\n",
    "    # compute pca of points in the patch:\n",
    "    # center the patch around the mean:\n",
    "    pts_mean = patch_points.mean(0)\n",
    "    patch_points = patch_points - pts_mean\n",
    "    trans, _, _ = torch.svd(torch.t(patch_points))\n",
    "    patch_points = torch.mm(patch_points, trans)\n",
    "    cp_new = -pts_mean  # since the patch was originally centered, the original cp was at (0,0,0)\n",
    "    cp_new = torch.matmul(cp_new, trans)\n",
    "    # re-center on original center point\n",
    "    patch_points = patch_points - cp_new\n",
    "    return patch_points, trans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_neighborhood_to_txt(patch_points, filename=\"neighborhood.txt\"):\n",
    "    np.savetxt(filename, patch_points, fmt=\"%.6f\", delimiter=\" \")\n",
    "    print(f\"Saved neighborhood to {filename}\")\n",
    "    \n",
    "    \n",
    "## Modified leihui code to save the files \n",
    "    \n",
    "def processPartL(kdtree, index, points, searchK):\n",
    "    # print (f'points[index, :]:{points[index, :]}')\n",
    "    point_distances, patch_point_inds = kdtree.query(points[index, :], k=searchK)\n",
    "    rad = max(point_distances)\n",
    "    patch_points = torch.from_numpy(points[patch_point_inds, :])\n",
    "    \n",
    "    # center the points around the query point and scale patch to unit sphere\n",
    "    patch_points = patch_points - torch.from_numpy(points[index, :])\n",
    "    # patch_points = patch_points / rad\n",
    "    \n",
    "    patch_points, trans = pca_points(patch_points)\n",
    "    return patch_points, patch_point_inds, trans, rad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PC-Diff imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcdiff import knn_graph, estimate_basis, build_grad_div\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_gradients(pointcloud, k_neighbors):\n",
    "    #TODO: Find out if this is only grad_x or just the gradient found here, maybe need more info for complete gradient\n",
    "    edge_index = knn_graph(pointcloud, k_neighbors)\n",
    "    normal, x_basis, y_basis = estimate_basis(pointcloud, edge_index)\n",
    "    grad, _ = build_grad_div(pointcloud, normal, x_basis, y_basis, edge_index)\n",
    "    x = np.random.rand(len(pointcloud), 1)\n",
    "    gradients = grad @ x\n",
    "    \n",
    "    \n",
    "    return gradients \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating function to generate datasets from pointclouds\n",
    "* Pointclouds in the dataset folder are all used, divided to use equal amount of neighborhoods from each cloud\n",
    "* Can be used to save multiple txt files containing a point cloud neighborhood each\n",
    "* Can be used to create one long txt file where the neighborhoods are appended to the file\n",
    "* The data will have the structure (x, y, z, gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(num_training_sets, num_neighbors, method = \"PCA\", datasets = \"./Data/Full_point_clouds\", save_path = \"./Data/Training_data\", save_to_file = False,\n",
    "                         single_file_name=\"CombinedDataset.txt\", save_mode=\"single\"):\n",
    "    \n",
    "    # check if the save folder is available or create it if not\n",
    "    if save_to_file and not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    all_files = [os.path.join(datasets, f) for f in os.listdir(datasets) if f.endswith('.txt') or f.endswith('.xyz')]\n",
    "    total_sets_created = 0 # Counter to see how many training points have been created \n",
    "    \n",
    "    sets_per_file = int(num_training_sets/len(all_files))\n",
    "    \n",
    "    for file_path in all_files:\n",
    "        if total_sets_created >= num_training_sets: \n",
    "            break # Break if the number of training data created is reached\n",
    "    \n",
    "    if save_mode == \"single\":\n",
    "        single_file_path = os.path.join(save_path, single_file_name)\n",
    "        if os.path.exists(single_file_path):\n",
    "            os.remove(single_file_path)  # Clear the file if it exists\n",
    "    \n",
    "        \n",
    "        \n",
    "    for file_path in all_files:\n",
    "        rawpoints = np.loadtxt(file_path, usecols=(0, 1, 2))  # Load only the first three columns (x, y, z)\n",
    "        print(f\"Loaded {rawpoints.shape[0]} points from {file_path}\")\n",
    "\n",
    "        points = preprocess(rawpoints)\n",
    "        points_np = np.array(points.block_until_ready())\n",
    "        kdtree = KDTree(points_np)\n",
    "\n",
    "        gradients = calculate_gradients(points_np, num_neighbors)\n",
    "        \n",
    "        selected_indices = random.sample(range(len(points)), sets_per_file)\n",
    "        \n",
    "        for i in selected_indices:\n",
    "            if method == \"PCA\":\n",
    "                neighborhood, indices, _, _ = processPartL(kdtree, i, points_np, num_neighbors)\n",
    "                \n",
    "                neighborhood_gradients = gradients[indices]\n",
    "                \n",
    "                neighborhood_with_gradients = np.hstack([neighborhood.numpy(), neighborhood_gradients])\n",
    "                \n",
    "                if save_to_file:\n",
    "                    if save_mode == \"multiple\":\n",
    "                        # Save each neighborhood in a separate file\n",
    "                        filename = os.path.join(save_path, f\"neighborhood_{total_sets_created}.txt\")\n",
    "                        save_neighborhood_to_txt(neighborhood_with_gradients, filename)\n",
    "                    elif save_mode == \"single\":\n",
    "                        # Append the neighborhood to a single file\n",
    "                        with open(single_file_path, \"a\") as f:\n",
    "                            np.savetxt(f, neighborhood_with_gradients, fmt=\"%.6f\", delimiter=\" \")\n",
    "\n",
    "                total_sets_created += 1\n",
    "\n",
    "                if total_sets_created >= num_training_sets:\n",
    "                    break  # Exits the inner loop\n",
    "\n",
    "    return neighborhood  # Align this correctly    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 35947 points from ./Data/Full_point_clouds\\bunny_order3_normal_beta.txt\n",
      "Loaded 120982 points from ./Data/Full_point_clouds\\cube-isometric.xyz\n"
     ]
    }
   ],
   "source": [
    "points = create_training_data(100, 20, save_to_file=True, save_mode=\"single\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
