{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit, vmap\n",
    "import torch\n",
    "from scipy.spatial import cKDTree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking for GPU, use CPU if no GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU: TFRT_CPU_0\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available\n",
    "if any(device.device_kind == 'gpu' for device in jax.devices()):\n",
    "    jax.config.update(\"jax_platform_name\", \"gpu\")\n",
    "    print(\"Running on GPU:\", jax.devices(\"gpu\")[0])\n",
    "# Use CPU if there is no GPU device available\n",
    "else:\n",
    "    jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "    print(\"Running on CPU:\", jax.devices(\"cpu\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading datasets to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points in dataset: 35947\n"
     ]
    }
   ],
   "source": [
    "# Load all datasets to use\n",
    "datasets = np.loadtxt(\"./Data/Full point clouds/bunny_order3_normal_beta.txt\"\n",
    "            # \"./Data/Full point clouds/etetwetwet.txt\"\n",
    "            # \"./Data/Full point clouds/etetwetwet.txt\"\n",
    "            )\n",
    "print(f\"Points in dataset: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def preprocess(points):\n",
    "    mean_p = points.mean(axis=0)\n",
    "    min_p, max_p = jnp.min(points, axis=0), jnp.max(points, axis=0)\n",
    "    bbdiag = jnp.linalg.norm(max_p - min_p, ord=2) # Bounding box diagonal L2 norm (Euclidean distance)\n",
    "    return (points - mean_p) / (0.5 * bbdiag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leihui code, for speed comparison \n",
    "def preprocessL(points):\n",
    "    bbdiag = float(np.linalg.norm(points.max(0) - points.min(0), 2))\n",
    "    points = (points - points.mean(0)) / (0.5*bbdiag)  # shrink shape to unit sphere\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Speed comparison of preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawpoints = datasets[:,0:3]\n",
    "\n",
    "# %timeit our = preprocess(rawpoints)\n",
    "# %timeit lei = preprocessL(rawpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our = preprocess(rawpoints)\n",
    "lei = preprocessL(rawpoints)\n",
    "\n",
    "\n",
    "print(jnp.allclose(lei, our, atol=1e-6))  # Equal down to a tolerance of 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_points(patch_points):\n",
    "    '''\n",
    "    Args:\n",
    "        patch_points: xyz points\n",
    "\n",
    "    Returns:\n",
    "        patch_points: xyz points after aligning using pca\n",
    "    '''\n",
    "    # compute pca of points in the patch:\n",
    "    # center the patch around the mean:\n",
    "    pts_mean = patch_points.mean(0)\n",
    "    patch_points = patch_points - pts_mean\n",
    "    trans, _, _ = torch.svd(torch.t(patch_points))\n",
    "    patch_points = torch.mm(patch_points, trans)\n",
    "    cp_new = -pts_mean  # since the patch was originally centered, the original cp was at (0,0,0)\n",
    "    cp_new = torch.matmul(cp_new, trans)\n",
    "    # re-center on original center point\n",
    "    patch_points = patch_points - cp_new\n",
    "    return patch_points, trans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_neighborhood_to_txt(patch_points, filename=\"neighborhood.txt\"):\n",
    "    np.savetxt(filename, patch_points.numpy(), fmt=\"%.6f\", delimiter=\" \")\n",
    "    print(f\"Saved neighborhood to {filename}\")\n",
    "    \n",
    "    \n",
    "## Modified leihui code to save the files \n",
    "    \n",
    "def processPartL(kdtree, index, points, searchK, save_to_file=False):\n",
    "    # print (f'points[index, :]:{points[index, :]}')\n",
    "    point_distances, patch_point_inds = kdtree.query(points[index, :], k=searchK)\n",
    "    rad = max(point_distances)\n",
    "    patch_points = torch.from_numpy(points[patch_point_inds, :])\n",
    "\n",
    "    # center the points around the query point and scale patch to unit sphere\n",
    "    patch_points = patch_points - torch.from_numpy(points[index, :])\n",
    "    # patch_points = patch_points / rad\n",
    "        # Save to file if required\n",
    "    if save_to_file:\n",
    "        save_neighborhood_to_txt(patch_points)  # No transpose needed    \n",
    "    \n",
    "    patch_points, trans = pca_points(patch_points)\n",
    "    return torch.transpose(patch_points, 0, 1), trans, rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved neighborhood to neighborhood.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000, -0.0075,  0.0075, -0.0113, -0.0038,  0.0038, -0.0151,  0.0150,\n",
       "           0.0114, -0.0037,  0.0036, -0.0188,  0.0189, -0.0114, -0.0227,  0.0225,\n",
       "           0.0112, -0.0263, -0.0149,  0.0266],\n",
       "         [ 0.0000,  0.0033, -0.0040, -0.0104, -0.0157,  0.0157,  0.0059, -0.0079,\n",
       "           0.0131,  0.0183, -0.0189, -0.0071,  0.0098,  0.0195,  0.0092, -0.0119,\n",
       "          -0.0229, -0.0038, -0.0227,  0.0086],\n",
       "         [ 0.0000, -0.0002,  0.0007, -0.0012,  0.0002, -0.0011,  0.0007,  0.0011,\n",
       "          -0.0015,  0.0001,  0.0007, -0.0015, -0.0021,  0.0016,  0.0008,  0.0016,\n",
       "           0.0010, -0.0013, -0.0042, -0.0036]]),\n",
       " tensor([[ 0.9539, -0.2405, -0.1793],\n",
       "         [-0.2996, -0.7321, -0.6119],\n",
       "         [-0.0159, -0.6374,  0.7704]]),\n",
       " np.float64(0.028196911705704875))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import KDTree\n",
    "import numpy as np\n",
    "\n",
    "# Example point cloud (replace with your real data)\n",
    "\n",
    "our_n = np.array(our.block_until_ready())\n",
    "kdtree = KDTree(our_n)\n",
    "\n",
    "\n",
    "\n",
    "# Select an index and find neighborhood\n",
    "index = 1000  # Query point index\n",
    "searchK = 20  # Number of neighbors\n",
    "\n",
    "# Process and save the neighborhood\n",
    "processPartL(kdtree, index, our_n, searchK, save_to_file=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
