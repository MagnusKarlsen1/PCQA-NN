{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network using FLAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax import jit, vmap\n",
    "import torch\n",
    "from scipy.spatial import cKDTree\n",
    "import optax\n",
    "from flax import nnx\n",
    "from optax import adam\n",
    "from tqdm.notebook import tqdm # progress bar\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_from_txt(txt, num_neighbors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: To jit nnx modules use @nnx.jitimport jax.numpy as jnp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get flax and optax up and running in the environment\n",
    "# TODO: Use a predefined loss function from optax\n",
    "# TODO: Use ADAM optimizer in FLAX model\n",
    "# TODO: Find out if we need to create a train_step and train_epoch function.\n",
    "#       - Create them if they are needed\n",
    "# TODO: Rewrite PCDiff to work in jax - should be relatively easy :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Model(nnx.Module):\n",
    "    def __init__(self,din,dout,rngs: nnx.Rngs):\n",
    "        self.linear1 = nnx.Linear(din,40,rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(40,20,rngs=rngs)\n",
    "        self.linear3 = nnx.Linear(20,10,rngs=rngs)\n",
    "        self.linear4 = nnx.Linear(10,dout,rngs=rngs)\n",
    "        \n",
    "    def __call__(self,x):\n",
    "        y = self.linear1(x)\n",
    "        y = nnx.relu(y)\n",
    "        y = self.linear2(y)\n",
    "        y = nnx.relu(y)\n",
    "        y = self.linear3(y)\n",
    "        y = nnx.relu(y)\n",
    "        y = self.linear4(y)\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.nnx as nnx\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "class SimpleGradientModel(nnx.Module):\n",
    "    def __init__(self):\n",
    "        self.dense1 = nnx.Linear(4, 32)\n",
    "        self.dense2 = nnx.Linear(32, 64)\n",
    "        self.dense3 = nnx.Linear(64, 32)\n",
    "        self.dense4 = nnx.Linear(32, 2)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = jnp.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = jnp.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = jnp.relu(x)\n",
    "        x = self.dense4(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "key = random.PRNGKey(0)\n",
    "x_sample = random.normal(key, (10, 20, 4))  # Example batch (10 sequences of 20 rows with 4 features each)\n",
    "\n",
    "model = SimpleGradientModel()\n",
    "params = model.init(key, x_sample)  # Initialize model\n",
    "output = model.apply(params, x_sample)  # Forward pass\n",
    "print(output.shape)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
